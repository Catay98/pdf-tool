import streamlit as st
import io
import os
import re
import base64
import tempfile
from PIL import Image
import pandas as pd
from collections import Counter
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

# åˆå§‹åŒ–NLTKèµ„æºï¼ˆé¦–æ¬¡è¿è¡Œæ—¶ä¸‹è½½ï¼‰
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# å®‰è£…å¹¶å¯¼å…¥å¿…è¦çš„PDFå¤„ç†åº“
import sys
try:
    import PyPDF2
except ImportError:
    os.system(f"{sys.executable} -m pip install PyPDF2==3.0.1")
    import PyPDF2

try:
    import pdfplumber
except ImportError:
    os.system(f"{sys.executable} -m pip install pdfplumber")
    import pdfplumber

try:
    import pytesseract
    from pdf2image import convert_from_bytes, convert_from_path
except ImportError:
    os.system(f"{sys.executable} -m pip install pytesseract pdf2image")
    import pytesseract
    from pdf2image import convert_from_bytes, convert_from_path

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="PDFçŸ¥è¯†ç‚¹æç‚¼å·¥å…·",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0D47A1;
    }
    .knowledge-point {
        background-color: #f8f9fa;
        border-left: 4px solid #4CAF50;
        padding: 10px;
        margin: 10px 0;
        border-radius: 4px;
    }
    .highlight {
        background-color: #fff3cd;
        font-weight: bold;
    }
    .debug-info {
        background-color: #f1f1f1;
        padding: 10px;
        border-radius: 4px;
        font-family: monospace;
        font-size: 0.8rem;
        max-height: 200px;
        overflow-y: auto;
    }
    .error-message {
        color: #d32f2f;
        background-color: #ffebee;
        padding: 10px;
        border-radius: 4px;
        margin: 10px 0;
    }
    .warning-message {
        color: #ff6f00;
        background-color: #fff8e1;
        padding: 10px;
        border-radius: 4px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

def get_tessdata_path():
    """å°è¯•å®šä½tesseractæ•°æ®ç›®å½•"""
    # æ£€æŸ¥æ˜¯å¦åœ¨Streamlit Cloudç¯å¢ƒ
    if os.path.exists('/mount/src'):
        # å°è¯•ä¸‹è½½ä¸­æ–‡å’Œè‹±æ–‡è¯­è¨€æ•°æ®åŒ…
        os.system("mkdir -p /tmp/tessdata")
        os.system("curl -L 'https://github.com/tesseract-ocr/tessdata_best/raw/main/chi_sim.traineddata' -o /tmp/tessdata/chi_sim.traineddata")
        os.system("curl -L 'https://github.com/tesseract-ocr/tessdata_best/raw/main/eng.traineddata' -o /tmp/tessdata/eng.traineddata")
        return "/tmp/tessdata"
    return None

def extract_text_with_ocr(pdf_file, pages=None, lang='chi_sim+eng'):
    """ä½¿ç”¨OCRä»PDFæå–æ–‡æœ¬"""
    try:
        # è®¾ç½®tessdataè·¯å¾„
        tessdata_path = get_tessdata_path()
        if tessdata_path:
            pytesseract.pytesseract.tesseract_cmd = 'tesseract'
            os.environ['TESSDATA_PREFIX'] = tessdata_path
            
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:
            temp_pdf.write(pdf_file.read())
            temp_pdf_path = temp_pdf.name
            
        pdf_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        
        # è½¬æ¢PDFä¸ºå›¾åƒ
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
            images = convert_from_path(temp_pdf_path, dpi=300, fmt='ppm', 
                                     thread_count=4, paths_only=False)
        except Exception as e:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å†…å­˜ä¸­çš„æ–‡ä»¶
            pdf_file.seek(0)
            images = convert_from_bytes(pdf_file.read(), dpi=300, fmt='ppm', 
                                      thread_count=4)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.unlink(temp_pdf_path)
        except:
            pass
        
        # æå–ç‰¹å®šé¡µé¢æˆ–æ‰€æœ‰é¡µé¢
        if pages:
            selected_images = [images[i-1] for i in pages if 0 < i <= len(images)]
        else:
            selected_images = images
            
        # ä½¿ç”¨OCRæå–æ–‡æœ¬
        text = ""
        for i, img in enumerate(selected_images):
            # ä½¿ç”¨pytesseractæ‰§è¡ŒOCR
            page_text = pytesseract.image_to_string(img, lang=lang)
            text += f"\n\n--- ç¬¬ {i+1} é¡µ ---\n\n" + page_text
            
        return text
        
    except Exception as e:
        st.error(f"OCRå¤„ç†é”™è¯¯: {str(e)}")
        return None

def extract_text_from_pdf(pdf_file, use_ocr=False, debug=False):
    """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬ï¼Œæ”¯æŒå¤šç§æ–¹æ³•"""
    results = {}
    errors = []
    text = ""
    
    # æ–¹æ³•1: ä½¿ç”¨PyPDF2
    try:
        pdf_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        reader = PyPDF2.PdfReader(pdf_file)
        pypdf2_text = ""
        for page in reader.pages:
            page_text = page.extract_text() or ""
            pypdf2_text += page_text + "\n\n"
        
        if pypdf2_text.strip():
            text = pypdf2_text
            results['PyPDF2'] = pypdf2_text
        else:
            errors.append("PyPDF2æœªèƒ½æå–ä»»ä½•æ–‡æœ¬")
    except Exception as e:
        errors.append(f"PyPDF2é”™è¯¯: {str(e)}")
    
    # æ–¹æ³•2: ä½¿ç”¨pdfplumber
    if not text.strip():
        try:
            pdf_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            with pdfplumber.open(pdf_file) as pdf:
                plumber_text = ""
                for page in pdf.pages:
                    page_text = page.extract_text() or ""
                    plumber_text += page_text + "\n\n"
                
                if plumber_text.strip():
                    text = plumber_text
                    results['pdfplumber'] = plumber_text
                else:
                    errors.append("pdfplumberæœªèƒ½æå–ä»»ä½•æ–‡æœ¬")
        except Exception as e:
            errors.append(f"pdfplumberé”™è¯¯: {str(e)}")
    
    # æ–¹æ³•3: å¦‚æœå‰ä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥æˆ–é€‰æ‹©äº†OCRï¼Œä½¿ç”¨OCR
    if (not text.strip() or use_ocr) and 'pytesseract' in sys.modules:
        try:
            pdf_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            ocr_text = extract_text_with_ocr(pdf_file)
            if ocr_text and ocr_text.strip():
                text = ocr_text
                results['OCR'] = ocr_text
            else:
                errors.append("OCRæœªèƒ½æå–ä»»ä½•æ–‡æœ¬")
        except Exception as e:
            errors.append(f"OCRé”™è¯¯: {str(e)}")
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
    if not text.strip():
        text = "æ— æ³•æå–æ–‡æœ¬ã€‚è¿™å¯èƒ½æ˜¯æ‰«æç‰ˆPDFæ²¡æœ‰æ–‡æœ¬å±‚ï¼Œæˆ–æ–‡ä»¶å—åˆ°ä¿æŠ¤ã€‚"
    
    # å¤„ç†ä¸€äº›å¸¸è§çš„PDFæå–é—®é¢˜
    if text.strip():
        # åˆ é™¤é‡å¤çš„è¡Œ
        lines = text.splitlines()
        cleaned_lines = []
        prev_line = ""
        for line in lines:
            if line != prev_line:
                cleaned_lines.append(line)
            prev_line = line
        text = "\n".join(cleaned_lines)
        
        # æ›¿æ¢ä¸€äº›å¸¸è§çš„ä¹±ç å­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\xff]', '', text)
        
        # å¦‚æœæå–çš„æ–‡æœ¬æ˜¯URLæˆ–ä¹±ç æ¨¡å¼ï¼Œå°è¯•OCR
        if re.search(r'(https?:\/\/[^\s]+){3,}', text) and 'OCR' not in results and 'pytesseract' in sys.modules:
            try:
                pdf_file.seek(0)
                ocr_text = extract_text_with_ocr(pdf_file)
                if ocr_text and ocr_text.strip() and len(ocr_text) > len(text):
                    text = ocr_text
                    results['OCR'] = ocr_text
            except Exception as e:
                errors.append(f"å°è¯•OCRä¿®å¤é”™è¯¯: {str(e)}")
    
    if debug:
        return text, results, errors
    return text

def preprocess_text(text):
    """é¢„å¤„ç†æ–‡æœ¬"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s\.\,\;\:\?\!ï¼Œã€‚ï¼›ï¼šï¼Ÿï¼ã€]', '', text)
    return text

def extract_sentences(text, min_length=5):
    """æå–æ–‡æœ¬ä¸­çš„å¥å­"""
    # å¤„ç†ä¸­æ–‡å’Œè‹±æ–‡æ··åˆå¥å­
    text = re.sub(r'([ã€‚ï¼ï¼Ÿï¼›!?;])', r'\1\n', text)
    sentences = []
    for line in text.split('\n'):
        if line.strip():
            # å¯¹äºè‹±æ–‡å’Œæ··åˆæ–‡æœ¬ä½¿ç”¨NLTK
            sentences.extend(sent_tokenize(line))
    
    # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
    return [s.strip() for s in sentences if len(s.strip().split()) >= min_length or len(s.strip()) >= 10]

def score_importance(sentence, keywords=None, stop_words=None, language='auto'):
    """è¯„ä¼°å¥å­çš„é‡è¦æ€§"""
    if language == 'auto':
        # ç®€å•æ£€æµ‹è¯­è¨€
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', sentence))
        if chinese_chars > len(sentence) / 3:
            language = 'chinese'
        else:
            language = 'english'
    
    if stop_words is None:
        try:
            if language == 'chinese':
                # ä¸­æ–‡åœç”¨è¯
                stop_words = set([
                    'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'è¿™', 'é‚£', 'æœ‰', 'åœ¨',
                    'ä¸­', 'ä¸º', 'å¯¹', 'ä¹Ÿ', 'ä»¥', 'äº', 'ä¸Š', 'ä¸‹', 'ä¹‹', 'ç”±', 'ç­‰', 'è¢«'
                ])
            else:
                stop_words = set(stopwords.words('english'))
        except:
            stop_words = set()
    
    # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯æ–¹æ³•
    if language == 'chinese':
        words = list(jieba.cut(sentence)) if 'jieba' in sys.modules else list(sentence)
    else:
        words = word_tokenize(sentence.lower())
    
    # è¿‡æ»¤åœç”¨è¯
    filtered_words = [w for w in words if w not in stop_words and (w.isalnum() or re.match(r'[\u4e00-\u9fff]', w))]
    
    # åŸºæœ¬åˆ†æ•°è®¡ç®—
    score = 0.5  # åŸºç¡€åˆ†æ•°
    
    # å¥å­é•¿åº¦å› ç´ 
    length = len(filtered_words)
    if length > 20:
        score += 0.1
    elif length > 10:
        score += 0.05
    
    # å…³é”®è¯åŒ¹é…
    if keywords:
        matches = sum(1 for word in filtered_words if word in keywords)
        score += 0.05 * matches
    
    # åŒ…å«æ•°å­—é€šå¸¸è¡¨ç¤ºæ›´å…·ä½“çš„ä¿¡æ¯
    if any(c.isdigit() for c in sentence):
        score += 0.05
    
    # ç‰¹æ®Šæ ‡è®°è¯è¯­ï¼Œé€šå¸¸è¡¨ç¤ºé‡è¦å†…å®¹
    importance_markers = {
        'english': ["important", "key", "significant", "essential", "crucial", "critical", "note", "remember"],
        'chinese': ["é‡è¦", "å…³é”®", "æ˜¾è‘—", "æœ¬è´¨", "è‡³å…³é‡è¦", "å…³é”®", "æ³¨æ„", "è®°ä½", "æ ¸å¿ƒ", "ä¸»è¦"]
    }
    
    markers = importance_markers.get(language, [])
    if any(marker in (words if language == 'chinese' else [w.lower() for w in words]) for marker in markers):
        score += 0.1
    
    # å¥å­ä½ç½®ç‰¹å¾ï¼ˆå¦‚æœæœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä¸­å¯èƒ½éœ€è¦æ®µè½çº§ä¿¡æ¯
    
    return min(score, 1.0)  # ç¡®ä¿åˆ†æ•°ä¸è¶…è¿‡1.0

def extract_keywords(text, top_n=20, language='auto'):
    """æå–æ–‡æœ¬ä¸­çš„å…³é”®è¯"""
    if language == 'auto':
        # ç®€å•æ£€æµ‹è¯­è¨€
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        if chinese_chars > len(text) / 3:
            language = 'chinese'
        else:
            language = 'english'
    
    try:
        if language == 'chinese':
            # å°è¯•ä½¿ç”¨jiebaæå–ä¸­æ–‡å…³é”®è¯
            try:
                import jieba.analyse
                keywords = jieba.analyse.extract_tags(text, topK=top_n)
                return keywords
            except:
                # ç®€å•æŒ‰è¯é¢‘æå–
                words = list(text)
                stop_words = set([
                    'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'è¿™', 'é‚£', 'æœ‰', 'åœ¨',
                    'ä¸­', 'ä¸º', 'å¯¹', 'ä¹Ÿ', 'ä»¥', 'äº', 'ä¸Š', 'ä¸‹', 'ä¹‹', 'ç”±', 'ç­‰', 'è¢«'
                ])
        else:
            # è‹±æ–‡å…³é”®è¯æå–
            stop_words = set(stopwords.words('english'))
            words = word_tokenize(text.lower())
            
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_words = [w for w in words if w not in stop_words and ((len(w) > 1 and language == 'chinese') or (len(w) > 2 and language != 'chinese'))]
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(filtered_words)
        
        # è¿”å›æœ€å¸¸è§çš„è¯
        return [word for word, _ in word_counts.most_common(top_n)]
    except Exception as e:
        st.warning(f"æå–å…³é”®è¯æ—¶å‡ºé”™: {str(e)}ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ–¹æ³•")
        # ç®€åŒ–æ–¹æ³•ï¼šæŒ‰è¯é¢‘æå–
        words = text.split() if language != 'chinese' else list(text)
        word_counts = Counter(words)
        return [word for word, _ in word_counts.most_common(top_n)]

def get_chapter_headings(text):
    """å°è¯•æå–ç« èŠ‚æ ‡é¢˜"""
    # åŒ¹é…å¸¸è§çš„ç« èŠ‚æ ‡é¢˜æ¨¡å¼
    patterns = [
        # è‹±æ–‡æ¨¡å¼
        r'(?:Chapter|CHAPTER)\s+\d+[:\.\s]+(.+?)(?:\n|\r\n?)',
        r'(?:Section|SECTION)\s+\d+(?:\.\d+)*[:\.\s]+(.+?)(?:\n|\r\n?)',
        r'^\d+(?:\.\d+)*[:\.\s]+(.+?)(?:\n|\r\n?)',
        # ä¸­æ–‡æ¨¡å¼
        r'ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\s*[ç« èŠ‚]\s*[ï¼š:]\s*(.+?)(?:\n|\r\n?)',
        r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[\.ã€]\s*(.+?)(?:\n|\r\n?)',
        r'[ï¼ˆ\(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ï¼‰\)]\s*(.+?)(?:\n|\r\n?)'
    ]
    
    headings = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.MULTILINE)
        headings.extend(matches)
    
    # å»é‡å¹¶æŒ‰åŸå§‹é¡ºåºä¿ç•™
    seen = set()
    unique_headings = []
    for h in headings:
        h_stripped = h.strip()
        if h_stripped and h_stripped not in seen and len(h_stripped) < 100:  # é¿å…åŒ¹é…è¿‡é•¿çš„å†…å®¹
            seen.add(h_stripped)
            unique_headings.append(h_stripped)
    
    return unique_headings

def create_downloadable_link(content, filename, link_text):
    """åˆ›å»ºå¯ä¸‹è½½å†…å®¹çš„é“¾æ¥"""
    b64 = base64.b64encode(content.encode()).decode()
    href = f'<a href="data:file/txt;base64,{b64}" download="{filename}" class="btn btn-primary">{link_text}</a>'
    return href

def extract_knowledge_points(text, mode, importance_threshold):
    """æ ¹æ®ä¸åŒæ¨¡å¼æå–çŸ¥è¯†ç‚¹"""
    text = preprocess_text(text)
    
    # æ£€æµ‹è¯­è¨€
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    language = 'chinese' if chinese_chars > len(text) / 10 else 'english'
    
    if mode == "è‡ªåŠ¨æ¨¡å¼":
        # æ£€æµ‹æ–‡æ¡£ç»“æ„ï¼Œé€‰æ‹©é€‚åˆçš„æ¨¡å¼
        headings = get_chapter_headings(text)
        if len(headings) > 2:
            mode = "ç« èŠ‚æ¨¡å¼"
        else:
            mode = "å¥å­æ¨¡å¼"
    
    # æå–å…³é”®è¯
    keywords = extract_keywords(text, language=language)
    
    if mode == "å…³é”®è¯æ¨¡å¼":
        # ç›´æ¥è¿”å›å…³é”®è¯ä½œä¸ºçŸ¥è¯†ç‚¹
        return [{"text": kw, "importance": 0.7, "type": "keyword"} for kw in keywords if kw]
    
    elif mode == "å¥å­æ¨¡å¼":
        # æå–å’Œè¯„åˆ†å¥å­
        sentences = extract_sentences(text)
        knowledge_points = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            importance = score_importance(sentence, keywords, language=language)
            if importance >= importance_threshold:
                knowledge_points.append({
                    "text": sentence,
                    "importance": importance,
                    "type": "sentence"
                })
        
        # æŒ‰é‡è¦æ€§é™åºæ’åº
        return sorted(knowledge_points, key=lambda x: x["importance"], reverse=True)
    
    elif mode == "ç« èŠ‚æ¨¡å¼":
        # æå–ç« èŠ‚æ ‡é¢˜å’Œç›¸å…³å†…å®¹
        headings = get_chapter_headings(text)
        
        if not headings:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç« èŠ‚æ ‡é¢˜ï¼Œå›é€€åˆ°å¥å­æ¨¡å¼
            st.warning("æœªæ‰¾åˆ°ç« èŠ‚æ ‡é¢˜ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°å¥å­æ¨¡å¼")
            return extract_knowledge_points(text, "å¥å­æ¨¡å¼", importance_threshold)
        
        sections = []
        
        # å°†æ–‡æœ¬æŒ‰ç« èŠ‚æ‹†åˆ†
        text_parts = []
        current_pos = 0
        
        for i, heading in enumerate(headings):
            # æŸ¥æ‰¾å½“å‰æ ‡é¢˜
            heading_pos = text.find(heading, current_pos)
            
            if heading_pos == -1:
                continue
                
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªæ ‡é¢˜ï¼Œå°†å‰ä¸€ä¸ªæ ‡é¢˜åˆ°å½“å‰æ ‡é¢˜ä¹‹é—´çš„å†…å®¹ä½œä¸ºå‰ä¸€ä¸ªæ ‡é¢˜çš„å†…å®¹
            if i > 0 and heading_pos > current_pos:
                text_parts.append(text[current_pos:heading_pos])
                
            current_pos = heading_pos + len(heading)
            
            # å¦‚æœæ˜¯æœ€åä¸€ä¸ªæ ‡é¢˜ï¼Œå°†å‰©ä½™å†…å®¹ä½œä¸ºæœ€åä¸€ä¸ªæ ‡é¢˜çš„å†…å®¹
            if i == len(headings) - 1:
                text_parts.append(text[current_pos:])
                
        # å¦‚æœæ‰¾åˆ°çš„ç« èŠ‚æ•°ä¸æ ‡é¢˜æ•°ä¸åŒ¹é…ï¼Œä½¿ç”¨ç®€å•çš„åˆ†å—æ–¹æ³•
        if len(text_parts) != len(headings):
            # ç®€å•åˆ†å—
            chunks = re.split(r'(ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\s*[ç« èŠ‚]|Chapter\s+\d+|Section\s+\d+(?:\.\d+)*)', text)
            # é‡å»ºæ–‡æœ¬éƒ¨åˆ†
            text_parts = []
            for i in range(1, len(chunks), 2):
                if i+1 < len(chunks):
                    text_parts.append(chunks[i+1])
                
            # å¦‚æœè¿˜æ˜¯ä¸åŒ¹é…ï¼Œä½¿ç”¨å¹³å‡åˆ†å—
            if len(text_parts) != len(headings):
                text_parts = []
                chunk_size = len(text) // len(headings)
                for i in range(len(headings)):
                    start = i * chunk_size
                    end = start + chunk_size if i < len(headings) - 1 else len(text)
                    text_parts.append(text[start:end])
        
        # å¤„ç†æ¯ä¸ªç« èŠ‚
        for i, heading in enumerate(headings):
            if i < len(text_parts):
                section_text = text_parts[i]
                # ä¸ºæ¯ä¸ªç« èŠ‚æå–å…³é”®å¥å­
                sentences = extract_sentences(section_text)
                section_points = []
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    importance = score_importance(sentence, keywords, language=language)
                    if importance >= importance_threshold:
                        section_points.append({
                            "text": sentence,
                            "importance": importance,
                            "type": "sentence"
                        })
                
                # æŒ‰é‡è¦æ€§é™åºæ’åº
                section_points = sorted(section_points, key=lambda x: x["importance"], reverse=True)
                max_points = min(10, len(section_points))  # æ¯ç« æœ€å¤š10ä¸ªè¦ç‚¹
                
                sections.append({
                    "heading": heading,
                    "importance": 0.9,  # ç« èŠ‚æ ‡é¢˜é€šå¸¸å¾ˆé‡è¦
                    "type": "heading",
                    "points": section_points[:max_points]
                })
        
        return sections

def format_knowledge_points(knowledge_points, mode, output_format):
    """æ ¼å¼åŒ–çŸ¥è¯†ç‚¹ä¸ºæŒ‡å®šæ ¼å¼"""
    if mode == "ç« èŠ‚æ¨¡å¼":
        if output_format == "Markdown":
            content = "# PDFæ–‡æ¡£çŸ¥è¯†ç‚¹æç‚¼\n\n"
            
            for section in knowledge_points:
                content += f"## {section['heading']}\n\n"
                
                for point in section['points']:
                    content += f"- {point['text']}\n\n"
            
        else:  # çº¯æ–‡æœ¬
            content = "PDFæ–‡æ¡£çŸ¥è¯†ç‚¹æç‚¼\n\n"
            
            for section in knowledge_points:
                content += f"{section['heading']}\n"
                content += "=" * len(section['heading']) + "\n\n"
                
                for i, point in enumerate(section['points']):
                    content += f"{i+1}. {point['text']}\n\n"
    else:
        # å…³é”®è¯æ¨¡å¼æˆ–å¥å­æ¨¡å¼
        if output_format == "Markdown":
            content = "# PDFæ–‡æ¡£çŸ¥è¯†ç‚¹æç‚¼\n\n"
            
            for i, point in enumerate(knowledge_points):
                if point['type'] == "keyword":
                    content += f"- **{point['text']}**\n"
                else:
                    content += f"## çŸ¥è¯†ç‚¹ {i+1}\n\n{point['text']}\n\n"
        else:  # çº¯æ–‡æœ¬
            content = "PDFæ–‡æ¡£çŸ¥è¯†ç‚¹æç‚¼\n\n"
            
            for i, point in enumerate(knowledge_points):
                if point['type'] == "keyword":
                    content += f"* {point['text']}\n"
                else:
                    content += f"{i+1}. {point['text']}\n\n"
    
    return content

# ä¸»ç•Œé¢
st.markdown('<h1 class="main-header">PDFçŸ¥è¯†ç‚¹æç‚¼å·¥å…·</h1>', unsafe_allow_html=True)
st.markdown('ä¸Šä¼ PDFæ–‡ä»¶ï¼Œè‡ªåŠ¨æå–é‡è¦çŸ¥è¯†ç‚¹ï¼Œå¸®åŠ©æ‚¨å¿«é€ŸæŒæ¡æ–‡æ¡£å†…å®¹ã€‚')

# ä¾§è¾¹æ å‚æ•°è®¾ç½®
with st.sidebar:
    st.header("å‚æ•°è®¾ç½®")
    
    importance = st.slider(
        "é‡è¦æ€§é˜ˆå€¼", 
        min_value=0.1, 
        max_value=0.9, 
        value=0.5, 
        step=0.1,
        help="è°ƒæ•´è¯¥å€¼æ§åˆ¶æå–çŸ¥è¯†ç‚¹çš„æ•°é‡ã€‚å€¼è¶Šå¤§ï¼Œæå–çš„çŸ¥è¯†ç‚¹è¶Šå°‘ä½†æ›´é‡è¦ã€‚"
    )
    
    mode = st.selectbox(
        "æå–æ¨¡å¼",
        ["è‡ªåŠ¨æ¨¡å¼", "å…³é”®è¯æ¨¡å¼", "å¥å­æ¨¡å¼", "ç« èŠ‚æ¨¡å¼"],
        help="è‡ªåŠ¨æ¨¡å¼ï¼šåˆ†ææ–‡æ¡£ç»“æ„é€‰æ‹©æœ€ä½³æå–æ–¹å¼\nå…³é”®è¯æ¨¡å¼ï¼šæå–é‡è¦æœ¯è¯­å’Œå…³é”®è¯\nå¥å­æ¨¡å¼ï¼šæå–é‡è¦å¥å­\nç« èŠ‚æ¨¡å¼ï¼šæŒ‰ç« èŠ‚ç»„ç»‡æå–"
    )
    
    output_format = st.selectbox(
        "è¾“å‡ºæ ¼å¼",
        ["Markdown", "çº¯æ–‡æœ¬"],
        help="Markdownï¼šæ ¼å¼åŒ–æ–‡æœ¬ï¼Œæ”¯æŒå±‚æ¬¡ç»“æ„\nçº¯æ–‡æœ¬ï¼šç®€å•æ–‡æœ¬æ ¼å¼"
    )
    
    # é«˜çº§é€‰é¡¹
    with st.expander("é«˜çº§é€‰é¡¹"):
        use_ocr = st.checkbox("å¯ç”¨OCRï¼ˆå¯¹æ‰«æç‰ˆPDFï¼‰", value=True, 
                            help="å¯¹äºæ‰«æç‰ˆPDFæˆ–æ— æ³•ç›´æ¥æå–æ–‡æœ¬çš„PDFä½¿ç”¨å…‰å­¦å­—ç¬¦è¯†åˆ«")
        
        show_debug = st.checkbox("æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯", value=False,
                               help="æ˜¾ç¤ºåŸå§‹æå–æ–‡æœ¬å’Œå¤„ç†è¿‡ç¨‹")
    
    st.divider()
    
    st.write("**å…³äºæœ¬å·¥å…·**")
    st.write("æœ¬å·¥å…·å¸®åŠ©æ‚¨ä»PDFæ–‡æ¡£ä¸­æå–é‡è¦çŸ¥è¯†ç‚¹ï¼Œæ”¯æŒæ–‡æœ¬PDFå’Œæ‰«æç‰ˆPDFã€‚")
    st.write("æå–çš„çŸ¥è¯†ç‚¹æŒ‰é‡è¦æ€§æ’åºï¼Œå¯å¸®åŠ©æ‚¨å¿«é€ŸæŒæ¡æ–‡æ¡£æ ¸å¿ƒå†…å®¹ã€‚")

# ä¸Šä¼ PDFæ–‡ä»¶
uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")

if uploaded_file is not None:
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    file_details = {
        "æ–‡ä»¶å": uploaded_file.name,
        "æ–‡ä»¶å¤§å°": f"{uploaded_file.size / 1024:.1f} KB"
    }
    st.write(file_details)
    
    # å¤„ç†æŒ‰é’®
    if st.button("å¼€å§‹æå–çŸ¥è¯†ç‚¹"):
        with st.spinner("æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ç¨å€™..."):
            try:
                # æå–æ–‡æœ¬
                if show_debug:
                    text, results, errors = extract_text_from_pdf(uploaded_file, use_ocr=use_ocr, debug=True)
                else:
                    text = extract_text_from_pdf(uploaded_file, use_ocr=use_ocr)
                
                # è°ƒè¯•ä¿¡æ¯
                if show_debug:
                    st.subheader("è°ƒè¯•ä¿¡æ¯")
                    st.markdown("#### æå–æ–¹æ³•")
                    for method, result in results.items():
                        with st.expander(f"{method} æå–ç»“æœ"):
                            st.markdown(f'<div class="debug-info">{result[:2000]}{"..." if len(result) > 2000 else ""}</div>', unsafe_allow_html=True)
                    
                    if errors:
                        st.markdown("#### é”™è¯¯ä¿¡æ¯")
                        for error in errors:
                            st.markdown(f'<div class="error-message">{error}</div>', unsafe_allow_html=True)
                    
                    st.markdown("#### åŸå§‹æå–æ–‡æœ¬")
                    st.markdown(f'<div class="debug-info">{text[:3000]}{"..." if len(text) > 3000 else ""}</div>', unsafe_allow_html=True)
                
                # å¦‚æœæ–‡æœ¬æå–æˆåŠŸ
                if text and not text.startswith("æ— æ³•æå–æ–‡æœ¬"):
                    # æå–çŸ¥è¯†ç‚¹
                    knowledge_points = extract_knowledge_points(text, mode, importance)
                    
                    if not knowledge_points or (mode != "ç« èŠ‚æ¨¡å¼" and len(knowledge_points) == 0) or \
                       (mode
