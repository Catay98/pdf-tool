import streamlit as st
import PyPDF2
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import re
import pandas as pd
from collections import Counter
import base64

# åˆå§‹åŒ–NLTKèµ„æºï¼ˆé¦–æ¬¡è¿è¡Œæ—¶ä¸‹è½½ï¼‰
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="PDFçŸ¥è¯†ç‚¹æç‚¼å·¥å…·",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0D47A1;
    }
    .knowledge-point {
        background-color: #f8f9fa;
        border-left: 4px solid #4CAF50;
        padding: 10px;
        margin: 10px 0;
        border-radius: 4px;
    }
    .highlight {
        background-color: #fff3cd;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

def preprocess_text(text):
    """é¢„å¤„ç†æ–‡æœ¬"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s\.\,\;\:\?\!]', '', text)
    return text

def extract_sentences(text, min_length=5):
    """æå–æ–‡æœ¬ä¸­çš„å¥å­"""
    sentences = sent_tokenize(text)
    # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
    return [s for s in sentences if len(word_tokenize(s)) >= min_length]

def score_importance(sentence, keywords=None, stop_words=None):
    """è¯„ä¼°å¥å­çš„é‡è¦æ€§"""
    if stop_words is None:
        try:
            stop_words = set(stopwords.words('english'))
        except:
            stop_words = set()
    
    words = word_tokenize(sentence.lower())
    # è¿‡æ»¤åœç”¨è¯
    filtered_words = [w for w in words if w not in stop_words and w.isalnum()]
    
    # åŸºæœ¬åˆ†æ•°è®¡ç®—
    score = 0.5  # åŸºç¡€åˆ†æ•°
    
    # å¥å­é•¿åº¦å› ç´ 
    length = len(filtered_words)
    if length > 20:
        score += 0.1
    elif length > 10:
        score += 0.05
    
    # å…³é”®è¯åŒ¹é…
    if keywords:
        matches = sum(1 for word in filtered_words if word in keywords)
        score += 0.05 * matches
    
    # åŒ…å«æ•°å­—é€šå¸¸è¡¨ç¤ºæ›´å…·ä½“çš„ä¿¡æ¯
    if any(w.isdigit() for w in words):
        score += 0.05
    
    # ç‰¹æ®Šæ ‡è®°è¯è¯­ï¼Œé€šå¸¸è¡¨ç¤ºé‡è¦å†…å®¹
    importance_markers = ["important", "key", "significant", "essential", "crucial", "critical"]
    if any(marker in filtered_words for marker in importance_markers):
        score += 0.1
    
    return min(score, 1.0)  # ç¡®ä¿åˆ†æ•°ä¸è¶…è¿‡1.0

def extract_keywords(text, top_n=20):
    """æå–æ–‡æœ¬ä¸­çš„å…³é”®è¯"""
    try:
        stop_words = set(stopwords.words('english'))
    except:
        stop_words = set()
        
    words = word_tokenize(text.lower())
    filtered_words = [w for w in words if w not in stop_words and w.isalnum() and len(w) > 2]
    
    # ç»Ÿè®¡è¯é¢‘
    word_counts = Counter(filtered_words)
    
    # è¿”å›æœ€å¸¸è§çš„è¯
    return [word for word, _ in word_counts.most_common(top_n)]

def get_chapter_headings(text):
    """å°è¯•æå–ç« èŠ‚æ ‡é¢˜"""
    # åŒ¹é…å¸¸è§çš„ç« èŠ‚æ ‡é¢˜æ¨¡å¼
    patterns = [
        r'(?:Chapter|CHAPTER)\s+\d+[:\.\s]+(.+?)(?:\n|\r\n?)',
        r'(?:Section|SECTION)\s+\d+(?:\.\d+)*[:\.\s]+(.+?)(?:\n|\r\n?)',
        r'^\d+(?:\.\d+)*[:\.\s]+(.+?)(?:\n|\r\n?)'
    ]
    
    headings = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.MULTILINE)
        headings.extend(matches)
    
    return headings

def create_downloadable_link(content, filename, link_text):
    """åˆ›å»ºå¯ä¸‹è½½å†…å®¹çš„é“¾æ¥"""
    b64 = base64.b64encode(content.encode()).decode()
    href = f'<a href="data:file/txt;base64,{b64}" download="{filename}">{link_text}</a>'
    return href

def extract_knowledge_points(text, mode, importance_threshold):
    """æ ¹æ®ä¸åŒæ¨¡å¼æå–çŸ¥è¯†ç‚¹"""
    text = preprocess_text(text)
    
    if mode == "è‡ªåŠ¨æ¨¡å¼":
        # æ£€æµ‹æ–‡æ¡£ç»“æ„ï¼Œé€‰æ‹©é€‚åˆçš„æ¨¡å¼
        headings = get_chapter_headings(text)
        if len(headings) > 3:
            mode = "ç« èŠ‚æ¨¡å¼"
        else:
            mode = "å¥å­æ¨¡å¼"
    
    # æå–å…³é”®è¯
    keywords = extract_keywords(text)
    
    if mode == "å…³é”®è¯æ¨¡å¼":
        # ç›´æ¥è¿”å›å…³é”®è¯ä½œä¸ºçŸ¥è¯†ç‚¹
        return [{"text": kw, "importance": 0.7, "type": "keyword"} for kw in keywords]
    
    elif mode == "å¥å­æ¨¡å¼":
        # æå–å’Œè¯„åˆ†å¥å­
        sentences = extract_sentences(text)
        knowledge_points = []
        
        for sentence in sentences:
            importance = score_importance(sentence, keywords)
            if importance >= importance_threshold:
                knowledge_points.append({
                    "text": sentence,
                    "importance": importance,
                    "type": "sentence"
                })
        
        # æŒ‰é‡è¦æ€§é™åºæ’åº
        return sorted(knowledge_points, key=lambda x: x["importance"], reverse=True)
    
    elif mode == "ç« èŠ‚æ¨¡å¼":
        # æå–ç« èŠ‚æ ‡é¢˜å’Œç›¸å…³å†…å®¹
        headings = get_chapter_headings(text)
        sections = []
        
        # ç®€å•åˆ†å—
        chunks = re.split(r'(?:Chapter|CHAPTER|Section|SECTION)\s+\d+', text)
        
        # å¤„ç†æ¯ä¸ªç« èŠ‚
        for i, heading in enumerate(headings):
            section_text = chunks[i+1] if i+1 < len(chunks) else ""
            # ä¸ºæ¯ä¸ªç« èŠ‚æå–å…³é”®å¥å­
            sentences = extract_sentences(section_text)
            section_points = []
            
            for sentence in sentences:
                importance = score_importance(sentence, keywords)
                if importance >= importance_threshold:
                    section_points.append({
                        "text": sentence,
                        "importance": importance,
                        "type": "sentence"
                    })
            
            # æŒ‰é‡è¦æ€§é™åºæ’åº
            section_points = sorted(section_points, key=lambda x: x["importance"], reverse=True)
            
            sections.append({
                "heading": heading,
                "importance": 0.9,  # ç« èŠ‚æ ‡é¢˜é€šå¸¸å¾ˆé‡è¦
                "type": "heading",
                "points": section_points[:5]  # æ¯ç« å–å‰5ä¸ªé‡è¦å¥å­
            })
        
        return sections

def format_knowledge_points(knowledge_points, mode, output_format):
    """æ ¼å¼åŒ–çŸ¥è¯†ç‚¹ä¸ºæŒ‡å®šæ ¼å¼"""
    if mode == "ç« èŠ‚æ¨¡å¼":
        if output_format == "Markdown":
            content = "# PDFæ–‡æ¡£çŸ¥è¯†ç‚¹æç‚¼\n\n"
            
            for section in knowledge_points:
                content += f"## {section['heading']}\n\n"
                
                for point in section['points']:
                    content += f"- {point['text']}\n"
                
                content += "\n"
        else:  # çº¯æ–‡æœ¬
            content = "PDFæ–‡æ¡£çŸ¥è¯†ç‚¹æç‚¼\n\n"
            
            for section in knowledge_points:
                content += f"{section['heading']}\n"
                content += "=" * len(section['heading']) + "\n\n"
                
                for i, point in enumerate(section['points']):
                    content += f"{i+1}. {point['text']}\n"
                
                content += "\n"
    else:
        # å…³é”®è¯æ¨¡å¼æˆ–å¥å­æ¨¡å¼
        if output_format == "Markdown":
            content = "# PDFæ–‡æ¡£çŸ¥è¯†ç‚¹æç‚¼\n\n"
            
            for i, point in enumerate(knowledge_points):
                if point['type'] == "keyword":
                    content += f"- **{point['text']}**\n"
                else:
                    content += f"## çŸ¥è¯†ç‚¹ {i+1}\n\n{point['text']}\n\n"
        else:  # çº¯æ–‡æœ¬
            content = "PDFæ–‡æ¡£çŸ¥è¯†ç‚¹æç‚¼\n\n"
            
            for i, point in enumerate(knowledge_points):
                if point['type'] == "keyword":
                    content += f"* {point['text']}\n"
                else:
                    content += f"{i+1}. {point['text']}\n\n"
    
    return content

# ä¸»ç•Œé¢
st.markdown('<h1 class="main-header">PDFçŸ¥è¯†ç‚¹æç‚¼å·¥å…·</h1>', unsafe_allow_html=True)
st.markdown('ä¸Šä¼ PDFæ–‡ä»¶ï¼Œè‡ªåŠ¨æå–é‡è¦çŸ¥è¯†ç‚¹ï¼Œå¸®åŠ©æ‚¨å¿«é€ŸæŒæ¡æ–‡æ¡£å†…å®¹ã€‚')

# ä¾§è¾¹æ å‚æ•°è®¾ç½®
with st.sidebar:
    st.header("å‚æ•°è®¾ç½®")
    
    importance = st.slider(
        "é‡è¦æ€§é˜ˆå€¼", 
        min_value=0.1, 
        max_value=0.9, 
        value=0.5, 
        step=0.1,
        help="è°ƒæ•´è¯¥å€¼æ§åˆ¶æå–çŸ¥è¯†ç‚¹çš„æ•°é‡ã€‚å€¼è¶Šå¤§ï¼Œæå–çš„çŸ¥è¯†ç‚¹è¶Šå°‘ä½†æ›´é‡è¦ã€‚"
    )
    
    mode = st.selectbox(
        "æå–æ¨¡å¼",
        ["è‡ªåŠ¨æ¨¡å¼", "å…³é”®è¯æ¨¡å¼", "å¥å­æ¨¡å¼", "ç« èŠ‚æ¨¡å¼"],
        help="è‡ªåŠ¨æ¨¡å¼ï¼šåˆ†ææ–‡æ¡£ç»“æ„é€‰æ‹©æœ€ä½³æå–æ–¹å¼\nå…³é”®è¯æ¨¡å¼ï¼šæå–é‡è¦æœ¯è¯­å’Œå…³é”®è¯\nå¥å­æ¨¡å¼ï¼šæå–é‡è¦å¥å­\nç« èŠ‚æ¨¡å¼ï¼šæŒ‰ç« èŠ‚ç»„ç»‡æå–"
    )
    
    output_format = st.selectbox(
        "è¾“å‡ºæ ¼å¼",
        ["Markdown", "çº¯æ–‡æœ¬"],
        help="Markdownï¼šæ ¼å¼åŒ–æ–‡æœ¬ï¼Œæ”¯æŒå±‚æ¬¡ç»“æ„\nçº¯æ–‡æœ¬ï¼šç®€å•æ–‡æœ¬æ ¼å¼"
    )
    
    st.divider()
    
    st.write("**å…³äºæœ¬å·¥å…·**")
    st.write("æœ¬å·¥å…·å¸®åŠ©æ‚¨ä»PDFæ–‡æ¡£ä¸­æå–é‡è¦çŸ¥è¯†ç‚¹ï¼ŒèŠ‚çœé˜…è¯»æ—¶é—´ã€‚")
    st.write("ç”±äºæ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œæå–æ•ˆæœå¯èƒ½ä¸å¤Ÿç²¾ç¡®ï¼Œä»…ä¾›å‚è€ƒã€‚")

# ä¸Šä¼ PDFæ–‡ä»¶
uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")

if uploaded_file is not None:
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    file_details = {
        "æ–‡ä»¶å": uploaded_file.name,
        "æ–‡ä»¶å¤§å°": f"{uploaded_file.size / 1024:.1f} KB"
    }
    st.write(file_details)
    
    # å¤„ç†æŒ‰é’®
    if st.button("å¼€å§‹æå–çŸ¥è¯†ç‚¹"):
        with st.spinner("æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ç¨å€™..."):
            try:
                # è¯»å–PDF
                reader = PyPDF2.PdfReader(uploaded_file)
                
                # æå–æ–‡æœ¬
                text = ""
                total_pages = len(reader.pages)
                
                progress_bar = st.progress(0)
                for i, page in enumerate(reader.pages):
                    text += page.extract_text() or ""
                    progress_bar.progress((i + 1) / total_pages)
                
                # æå–çŸ¥è¯†ç‚¹
                knowledge_points = extract_knowledge_points(text, mode, importance)
                
                # æ ¼å¼åŒ–è¾“å‡º
                result_content = format_knowledge_points(knowledge_points, mode, output_format)
                
                # åˆ›å»ºç»“æœåŒºåŸŸ
                st.markdown('<h2 class="sub-header">æå–ç»“æœ</h2>', unsafe_allow_html=True)
                
                # æ˜¾ç¤ºçŸ¥è¯†ç‚¹
                if mode == "ç« èŠ‚æ¨¡å¼":
                    for section in knowledge_points:
                        with st.expander(f"{section['heading']}"):
                            for point in section['points']:
                                st.markdown(f'<div class="knowledge-point">{point["text"]}</div>', unsafe_allow_html=True)
                else:
                    # å…³é”®è¯æ¨¡å¼æˆ–å¥å­æ¨¡å¼
                    for point in knowledge_points:
                        if point['type'] == "keyword":
                            st.markdown(f'<span class="highlight">{point["text"]}</span> ', unsafe_allow_html=True)
                        else:
                            st.markdown(f'<div class="knowledge-point">{point["text"]}</div>', unsafe_allow_html=True)
                
                # ä¸‹è½½é“¾æ¥
                st.markdown("### ä¸‹è½½ç»“æœ")
                filename = f"{uploaded_file.name.split('.')[0]}_çŸ¥è¯†ç‚¹.{'md' if output_format == 'Markdown' else 'txt'}"
                download_link = create_downloadable_link(result_content, filename, "ç‚¹å‡»ä¸‹è½½çŸ¥è¯†ç‚¹æå–ç»“æœ")
                st.markdown(download_link, unsafe_allow_html=True)
                
                # ç»Ÿè®¡ä¿¡æ¯
                st.sidebar.success(f"æˆåŠŸæå–äº†{len(knowledge_points) if mode != 'ç« èŠ‚æ¨¡å¼' else sum(len(s['points']) for s in knowledge_points)}ä¸ªçŸ¥è¯†ç‚¹")
                
            except Exception as e:
                st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                st.info("æç¤ºï¼šå¦‚æœæ˜¯PDFæ ¼å¼é—®é¢˜ï¼Œè¯·å°è¯•ä½¿ç”¨å…¶ä»–PDFæ–‡ä»¶ã€‚")
else:
    # æœªä¸Šä¼ æ–‡ä»¶æ—¶æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    st.info("è¯·ä¸Šä¼ PDFæ–‡ä»¶ä»¥å¼€å§‹æå–çŸ¥è¯†ç‚¹")
    
    with st.expander("ä½¿ç”¨æŒ‡å—"):
        st.markdown("""
        ### åŸºæœ¬ä½¿ç”¨æµç¨‹
        
        1. åœ¨å·¦ä¾§ä¸Šä¼ ä¸€ä¸ªPDFæ–‡ä»¶ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        2. æ ¹æ®éœ€è¦è°ƒæ•´å‚æ•°ï¼š
           - **é‡è¦æ€§é˜ˆå€¼**ï¼šæ§åˆ¶æå–çš„çŸ¥è¯†ç‚¹æ•°é‡å’Œè´¨é‡
           - **æå–æ¨¡å¼**ï¼šé€‰æ‹©é€‚åˆæ‚¨æ–‡æ¡£çš„æå–æ–¹å¼
           - **è¾“å‡ºæ ¼å¼**ï¼šé€‰æ‹©ç»“æœçš„æ ¼å¼åŒ–æ–¹å¼
        3. ç‚¹å‡»"å¼€å§‹æå–çŸ¥è¯†ç‚¹"æŒ‰é’®
        4. æŸ¥çœ‹æå–ç»“æœå¹¶ä¸‹è½½
        
        ### æå–æ¨¡å¼è¯´æ˜
        
        - **è‡ªåŠ¨æ¨¡å¼**ï¼šç³»ç»Ÿåˆ†ææ–‡æ¡£ç»“æ„ï¼Œé€‰æ‹©æœ€åˆé€‚çš„æå–æ–¹å¼
        - **å…³é”®è¯æ¨¡å¼**ï¼šæå–æ–‡æ¡£ä¸­çš„é‡è¦æœ¯è¯­å’Œå…³é”®è¯
        - **å¥å­æ¨¡å¼**ï¼šæå–åŒ…å«é‡è¦ä¿¡æ¯çš„å®Œæ•´å¥å­
        - **ç« èŠ‚æ¨¡å¼**ï¼šæŒ‰æ–‡æ¡£ç« èŠ‚ç»“æ„ç»„ç»‡æå–çš„çŸ¥è¯†ç‚¹
        
        ### é‡è¦æ€§é˜ˆå€¼
        
        - **0.1-0.3**ï¼šæå–æ›´å¤šçŸ¥è¯†ç‚¹ï¼ŒåŒ…æ‹¬æ¬¡è¦å†…å®¹
        - **0.4-0.6**ï¼šå¹³è¡¡æ•°é‡å’Œè´¨é‡
        - **0.7-0.9**ï¼šä»…æå–æœ€é‡è¦çš„çŸ¥è¯†ç‚¹
        """)

# æ·»åŠ é¡µè„š
st.markdown("""
---
<p style="text-align: center; color: gray; font-size: 0.8em;">
PDFçŸ¥è¯†ç‚¹æç‚¼å·¥å…· | ç‰ˆæœ¬ 1.0 | Â© 2025
</p>
""", unsafe_allow_html=True)